# -*- coding: utf-8 -*-
"""dataloader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v5PntBtcfoucm-fzaFxfmDT4UhQjrXE9
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pickle
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

import torch
from torch.utils.data import Dataset, DataLoader


# === Global Settings ===
FILE_DIR = "/content/drive/MyDrive/BOAZ_mini2/"
RAW_FILE_NAME = "final_df_3.csv"  # 1ì°¨ ì „ì²˜ë¦¬í•œ ì›ë³¸ ë°ì´í„°
SAVE_NAME = "ID_Index_mappings.pkl"  # ID-Index ë§¤í•‘í…Œì´ë¸”
PREPROCESSED_FILENAME = "final_preprocessed_df.parquet" # ìµœì¢… ì „ì²˜ë¦¬ íŒŒì¼ (train/test ë¶„ë¦¬ ì´ì „)

# === Train / Valid / Test Split Ratios (ì „ì—­ íŒŒë¼ë¯¸í„°) ===
TRAIN_RATIO = 0.8
VALID_RATIO = 0.1
TEST_RATIO  = 0.1

# === DataLoader / Target ê´€ë ¨ ì „ì—­ íŒŒë¼ë¯¸í„° ===
TARGET_COL = "target"
BATCH_SIZE = 1024
NUM_WORKERS = 4
PIN_MEMORY = True
SHUFFLE_TRAIN = True

"""ë°ì´í„° ì‹œê°„ ìˆœ ì •ë ¬"""

def sort_by_event_time(df):
    """
    event_time ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬í•˜ëŠ” í•¨ìˆ˜.
    - event_timeì´ ë¬¸ìì—´ì¸ ê²½ìš° datetimeìœ¼ë¡œ ë³€í™˜ í›„ ì •ë ¬
    """

    # datetime ë³€í™˜ (ë¬¸ìì—´ì¼ ê²½ìš° ëŒ€ë¹„)
    if not pd.api.types.is_datetime64_any_dtype(df["event_time"]):
        df["event_time"] = pd.to_datetime(df["event_time"], errors="coerce")

    # ì •ë ¬
    df = df.sort_values(by="event_time").reset_index(drop=True)

    return df

"""ì»¬ëŸ¼ ì •ë¦¬"""

def drop_event_time(df):
    # event_time â†’ ì´ë¯¸ hour/day_of_week/is_weekend ë¡œ íŒŒìƒë˜ì—ˆìœ¼ë¯€ë¡œ ì œê±°
    if 'event_time' in df.columns:
        df = df.drop(columns=['event_time'])
    return df

def validate_and_cleanup_event_columns(df):
    """
    1. event_type â†’ ìˆ«ì ì½”ë“œë¡œ ë³€í™˜ (view=0, cart=1, purchase=2)
    2. ì´ ìˆ«ìê°€ event_label, targetê³¼ ëª¨ë‘ ë™ì¼í•œì§€ ê²€ì¦
    3. ë™ì¼í•˜ë©´ targetë§Œ ë‚¨ê¸°ê³  event_type, event_label ì œê±°
    """

    # 1) ë¬¸ìì—´ â†’ ìˆ«ì ë§¤í•‘
    mapping = {
        "view": 0,
        "cart": 1,
        "purchase": 2
    }

    # ìƒˆ ì»¬ëŸ¼ ìƒì„±
    df["event_type_code"] = df["event_type"].map(mapping)

    # event_type_codeê°€ ì œëŒ€ë¡œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ ì²´í¬
    if df["event_type_code"].isnull().any():
        raise ValueError("event_typeì— ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ìì—´ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    # 2) event_label, target ê³¼ ê°™ì€ì§€ í™•ì¸
    cond_label_match  = (df["event_type_code"] == df["event_label"]).all()
    cond_target_match = (df["event_type_code"] == df["target"]).all()

    print("event_type_code vs event_label ë™ì¼? :", cond_label_match)
    print("event_type_code vs target ë™ì¼?      :", cond_target_match)

    # 3) ë‘˜ ë‹¤ ëª¨ë‘ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ ì»¬ëŸ¼ ì œê±°
    if cond_label_match and cond_target_match:
        print("âœ” ëª¨ë“  ê°’ì´ ë™ì¼í•˜ë¯€ë¡œ targetë§Œ ë‚¨ê¸°ê³  event_type, event_label ì œê±°í•©ë‹ˆë‹¤.")
        df = df.drop(columns=["event_type", "event_label", "event_type_code"])
    else:
        print("âœ˜ ì„¸ ì»¬ëŸ¼ì´ ë™ì¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì§ì ‘ ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.")
        print("ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ì˜ˆì‹œ:")
        mismatched = df[(df["event_type_code"] != df["event_label"]) |
                        (df["event_type_code"] != df["target"])].head()
        print(mismatched)
        # ì—¬ê¸°ì„œ dropì€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ

    return df

def convert_target_to_softlabel(df):
    """
    target ê°’ì„ 0/1/2 â†’ 0/0.5/1 ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜.
    - 0(view)      â†’ 0.0
    - 1(cart)      â†’ 0.5
    - 2(purchase)  â†’ 1.0
    """
    mapping = {
        0: 0.0,
        1: 0.5,
        2: 1.0
    }

    # ì•ˆì „ì¥ì¹˜: ì˜ˆìƒ ë°– ê°’ì´ ìˆìœ¼ë©´ ì—ëŸ¬
    unique_vals = set(df["target"].unique())
    unexpected = unique_vals - set(mapping.keys())
    if unexpected:
        raise ValueError(f"targetì— ì˜ˆìƒì¹˜ ëª»í•œ ê°’ì´ ìˆìŠµë‹ˆë‹¤: {unexpected}")

    df["target"] = df["target"].map(mapping).astype(float)

    return df

"""object íƒ€ì… ì •ì œ"""

def clean_object_columns(df):
    """
    category_code, brand, user_session ê°™ì€ object íƒ€ì… ë¬¸ìì—´ì„
    ì¼ê´€ì„± ìˆê²Œ ì •ì œí•˜ëŠ” í•¨ìˆ˜.

    ì²˜ë¦¬ ë‚´ìš©:
    - NaN â†’ 'unknown'
    - ë¬¸ìì—´ë¡œ ë³€í™˜
    - ì•ë’¤ ê³µë°± ì œê±°
    - ì†Œë¬¸ìë¡œ í†µì¼
    """

    object_cols = ["category_code", "brand", "user_session"]

    for col in object_cols:
        df[col] = (
            df[col]
            .fillna("unknown")   # null ëŒ€ë¹„ (ì´ë¯¸ ì—†ì§€ë§Œ ì•ˆì „ì¥ì¹˜)
            .astype(str)         # ë¬¸ìì—´ë¡œ ê°•ì œ ë³€í™˜
            .str.strip()         # ì•ë’¤ ê³µë°± ì œê±°
            .str.lower()         # ì†Œë¬¸ìë¡œ í†µì¼
        )

    return df

"""ìˆ˜ì¹˜í˜• ë³€ìˆ˜"""

def scale_numerical_features(df):

    df = df.copy()

    PRICE_COL = "price"
    OTHER_NUMERIC = ["session_rank", "hour", "day_of_week"]  # is_weekend ì œì™¸

    # ------------------------
    # 1) price: log1p + MinMaxScaling
    # ------------------------
    if PRICE_COL in df.columns:
        df["price_log"] = np.log1p(df[PRICE_COL])
        scaler_price = MinMaxScaler()
        df["price_scaled"] = scaler_price.fit_transform(df[["price_log"]])
        df.drop(columns=[PRICE_COL, "price_log"], inplace=True)
        print("âœ… price: log1p + MinMaxScaling â†’ price_scaled ìƒì„±")

    # ------------------------
    # 2) ë‚˜ë¨¸ì§€ ìˆ˜ì¹˜í˜• (session_rank, hour, day_of_week)
    # ------------------------
    cols_to_scale = [col for col in OTHER_NUMERIC if col in df.columns]

    for col in cols_to_scale:
        scaler = MinMaxScaler()
        df[f"{col}_scaled"] = scaler.fit_transform(df[[col]])
        df.drop(columns=[col], inplace=True)

    if cols_to_scale:
        print(f"âœ… {cols_to_scale}: MinMaxScaling â†’ *_scaled ìƒì„±")

    # ------------------------
    # 3) is_weekendëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (ìŠ¤ì¼€ì¼ë§ X)
    # ------------------------
    print("â„¹ï¸ is_weekend: ë³€ê²½ ì—†ìŒ (0/1 ê·¸ëŒ€ë¡œ ìœ ì§€)")

    return df

"""ë²”ì£¼í˜• ë³€ìˆ˜ \
-> ID - Index ë§¤í•‘ í…Œì´ë¸” ìƒì„±( .pkl)
"""

def build_and_apply_mappings(df: pd.DataFrame, drop_original=True):
    """
    6ê°œ ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜(product_id, category_id, category_code, brand, user_id, user_session)ì— ëŒ€í•´
    ì›ë˜ê°’ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„± + dfì— *_idx ì»¬ëŸ¼ ìƒì„± + ë§¤í•‘ íŒŒì¼ ì €ì¥
    """

    # ì €ì¥ ê²½ë¡œ
    save_path = os.path.join(FILE_DIR, SAVE_NAME)

    # ë””ë ‰í† ë¦¬ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(FILE_DIR, exist_ok=True)

    cat_cols = [
        "product_id",
        "category_id",
        "category_code",
        "brand",
        "user_id",
        "user_session",
    ]

    # unknownì„ 0ë²ˆìœ¼ë¡œ ë³´ë‚´ê³  ì‹¶ì€ ì»¬ëŸ¼
    unknown_as_zero_cols = {"category_code", "brand"}

    mappings = {}

    for col in cat_cols:
        col_series = df[col].astype(str)
        uniques = pd.unique(col_series)

        col_map = {}
        next_idx = 0

        # 1) unknownì„ 0ë²ˆ ì¸ë±ìŠ¤ë¡œ ê³ ì •í•´ì•¼ í•˜ëŠ” ê²½ìš°
        if col in unknown_as_zero_cols and "unknown" in uniques:
            col_map["unknown"] = 0
            next_idx = 1
            uniques = [u for u in uniques if u != "unknown"]

        # 2) ë‚˜ë¨¸ì§€ ê°’ë“¤ ì¸ë±ìŠ¤ ë¶€ì—¬
        for v in uniques:
            col_map[v] = next_idx
            next_idx += 1

        mappings[col] = col_map

        # 3) dfì— ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì¶”ê°€
        df[col + "_idx"] = col_series.map(col_map).astype("int32")

    # 4) ì›ë³¸ ì»¬ëŸ¼ ì‚­ì œ ì—¬ë¶€
    if drop_original:
        df = df.drop(columns=cat_cols)

    # 5) ë§¤í•‘ í”¼í´ ì €ì¥
    with open(save_path, "wb") as f:
        pickle.dump(mappings, f)

    print(f"ë§¤í•‘ íŒŒì¼ ì €ì¥ë¨ â†’ {save_path}")
    return df, mappings

"""seq_prod ì»¬ëŸ¼ \
-> DeepFM, AutoInt ëª¨ë¸ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì§€ ëª»í•¨ \
-> ì»¬ëŸ¼ 5ê°œë¡œ ë¶„ë¦¬ \
-> ì¸ë±ì‹±ì€ product_idë¡œ ìƒì„±í•œ í…Œì´ë¸” ì´ìš©í•´ ë§¤ì¹­
"""

def apply_seq_prod_mapping(df, mappings, seq_col="seq_prod", max_len=5):
    """
    seq_prod ì»¬ëŸ¼(ê¸¸ì´ 5ì§œë¦¬ product_id ë¦¬ìŠ¤íŠ¸)ì„
    seq_prod_1_idx ~ seq_prod_5_idx ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜.

    ê·œì¹™:
    - seq_prod[0] = ê°€ì¥ ì˜¤ë˜ëœ ê³¼ê±° í–‰ë™ â†’ seq_prod_1_idx
    - ...
    - seq_prod[4] = ì§ì „ ê³¼ê±° í–‰ë™      â†’ seq_prod_5_idx
    - ê°’ì´ 0ì¸ ê³³(íŒ¨ë”©)ì€ product_id ë§¤í•‘ì— ì—†ìœ¼ë¯€ë¡œ ìë™ìœ¼ë¡œ 0ìœ¼ë¡œ ë‚¨ìŒ.
    """

    prod_map = mappings["product_id"]  # ê¸°ì¡´ product_id ë§¤í•‘ ì¬ì‚¬ìš©

    def to_list(seq):
        # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš°ë„ ëŒ€ë¹„
        if isinstance(seq, str):
            try:
                seq = ast.literal_eval(seq)
            except Exception:
                return []
        if isinstance(seq, (list, tuple)):
            return list(seq)
        return []

    seq_series = df[seq_col].apply(to_list)

    for i in range(max_len):
        new_col = f"{seq_col}_{i+1}_idx"   # seq_prod_1_idx ~ seq_prod_5_idx

        df[new_col] = seq_series.apply(
            lambda seq: (
                prod_map.get(str(seq[i]), 0)  # ië²ˆì§¸ product_id â†’ ì¸ë±ìŠ¤, ì—†ìœ¼ë©´ 0
                if len(seq) > i
                else 0                        # ë¦¬ìŠ¤íŠ¸ê°€ ë” ì§§ìœ¼ë©´ PADì²˜ëŸ¼ 0
            )
        ).astype("int32")

    # ğŸ”¥ ì—¬ê¸°ì„œ ì›ë³¸ seq_prod ì»¬ëŸ¼ ì‚­ì œ
    df = df.drop(columns=[seq_col])

    return df

"""ìµœì¢… ì „ì²˜ë¦¬ íŒŒì¼ ì €ì¥"""

def save_preprocessed_parquet(df, file_dir, filename="final_preprocessed_df.parquet"):
    """
    ì „ì²˜ë¦¬ ì™„ë£Œëœ DataFrameì„ parquet í˜•íƒœë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    - df: ì „ì²˜ë¦¬ ì™„ë£Œëœ DataFrame
    - file_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
    - filename: ì €ì¥í•  íŒŒì¼ ì´ë¦„ (ê¸°ë³¸ê°’: final_preprocessed_df.parquet)
    """
    # ê²½ë¡œ ìƒì„±
    os.makedirs(file_dir, exist_ok=True)
    save_path = os.path.join(file_dir, filename)

    # parquet ì €ì¥
    df.to_parquet(save_path, index=False)

    print(f"[INFO] Parquet ì €ì¥ ì™„ë£Œ: {save_path}")
    return save_path

"""$\text{Train}/\text{Test}$ ë¶„ë¦¬"""

def split_exact_tvt_from_preprocessed(
    random_state=42,
):
    """
    ì „ì—­ë³€ìˆ˜: FILE_DIR, PREPROCESSED_FILENAME, TRAIN_RATIO, VALID_RATIO, TEST_RATIOë¥¼ ì‚¬ìš©í•´
    parquet íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì½ê³  ì •í™•í•œ ë¹„ìœ¨ splitì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜.
    """

    # ë¹„ìœ¨ í•© í™•ì¸
    assert abs((TRAIN_RATIO + VALID_RATIO + TEST_RATIO) - 1.0) < 1e-6, \
        "TRAIN_RATIO + VALID_RATIO + TEST_RATIO must equal 1."

    # Load parquet
    parquet_path = os.path.join(FILE_DIR, PREPROCESSED_FILENAME)
    print(f"[INFO] Loading preprocessed parquet from: {parquet_path}")
    df = pd.read_parquet(parquet_path)

    n = len(df)

    # === Test: ìµœì‹  TEST_RATIO ë§Œí¼ ë’¤ìª½ ===
    test_start = int(n * (1 - TEST_RATIO))
    df_test = df.iloc[test_start:].reset_index(drop=True)
    df_early = df.iloc[:test_start].reset_index(drop=True)

    # === ì „ì²´ ê¸°ì¤€ ì •í™•í•œ train/valid ê°œìˆ˜ ===
    exact_train_n = int(n * TRAIN_RATIO)
    exact_valid_n = int(n * VALID_RATIO)

    # === df_earlyì—ì„œ train/valid ëœë¤ ìƒ˜í”Œë§ ===
    df_train = df_early.sample(n=exact_train_n, random_state=random_state)
    df_valid = df_early.drop(df_train.index).sample(n=exact_valid_n, random_state=random_state)

    # reset index
    df_train = df_train.reset_index(drop=True)
    df_valid = df_valid.reset_index(drop=True)

    print(f"[INFO] TOTAL: {n:,}")
    print(f"[INFO] Train: {len(df_train):,} ({TRAIN_RATIO*100:.1f}%)")
    print(f"[INFO] Valid: {len(df_valid):,} ({VALID_RATIO*100:.1f}%)")
    print(f"[INFO] Test : {len(df_test):,} ({TEST_RATIO*100:.1f}%)")

    return df_train, df_valid, df_test

"""Dataset ì •ì˜"""

# ==========================
# PyTorch Dataset ì •ì˜
# ==========================
class CTRDataset(Dataset):
    """
    - df: í•˜ë‚˜ì˜ split (train/valid/test)
    - feature_cols: TARGET_COLì„ ì œì™¸í•œ í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    - target_col : íƒ€ê²Ÿ ì»¬ëŸ¼ëª… (ì „ì—­ TARGET_COL ê¸°ë³¸ê°’)
    """

    def __init__(self, df: pd.DataFrame, feature_cols, target_col: str = TARGET_COL):
        self.df = df
        self.feature_cols = list(feature_cols)
        self.target_col = target_col

        # dtype ì²´í¬ (object/string ë‚¨ì•„ ìˆìœ¼ë©´ ê²½ê³ )
        obj_cols = df[self.feature_cols].select_dtypes(include=["object"]).columns.tolist()
        if len(obj_cols) > 0:
            print(f"[WARN] Object dtype feature columns detected: {obj_cols}")
            print("       ëª¨ë¸ ë„£ê¸° ì „ì— ëª¨ë‘ ì¸ë±ì‹±/ì¸ì½”ë”© ë˜ì–´ ìˆì–´ì•¼ í•¨!")

        # valuesë¡œ ë¯¸ë¦¬ ë½‘ì•„ë‘ê¸°
        self.X = self.df[self.feature_cols].values.astype(np.float32)
        self.y = self.df[self.target_col].values.astype(np.float32)  # soft label 0/0.5/1 ê¸°ì¤€

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        x = torch.from_numpy(self.X[idx])
        y = torch.tensor(self.y[idx], dtype=torch.float32)
        return x, y

"""DataLoader ìƒì„±"""

# ==========================
# DataLoader ìƒì„± í•¨ìˆ˜
# ==========================
def create_dataloaders_from_df(train_df, valid_df, test_df):
    """
    ì´ë¯¸ split_exact_tvt_from_preprocessed()ë¡œ ë‚˜ëˆˆ
    train_df, valid_df, test_dfë¥¼ ë°›ì•„ì„œ

    1) feature_cols = ì „ì²´ ì»¬ëŸ¼ - TARGET_COL
    2) CTRDataset ìƒì„±
    3) DataLoaderê¹Œì§€ ìƒì„±
    """

    # 1) feature ì»¬ëŸ¼ ìë™ ì¶”ì¶œ
    all_cols = train_df.columns.tolist()
    assert TARGET_COL in all_cols, f"[ERROR] TARGET_COL '{TARGET_COL}' not in dataframe columns."

    feature_cols = [c for c in all_cols if c != TARGET_COL]

    print(f"[INFO] Feature columns ({len(feature_cols)}): {feature_cols}")
    print(f"[INFO] Target column: {TARGET_COL}")

    # 2) Dataset ìƒì„±
    train_dataset = CTRDataset(train_df, feature_cols, TARGET_COL)
    valid_dataset = CTRDataset(valid_df, feature_cols, TARGET_COL)
    test_dataset  = CTRDataset(test_df,  feature_cols, TARGET_COL)

    # 3) DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=SHUFFLE_TRAIN,
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        drop_last=False,
    )

    valid_loader = DataLoader(
        valid_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        drop_last=False,
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        drop_last=False,
    )

    return train_loader, valid_loader, test_loader, feature_cols

"""ë©”ì¸ í•¨ìˆ˜"""

if __name__ == "__main__":
    # 0) CSV ê²½ë¡œ ì„¤ì • (ì „ì—­ FILE_DIR, RAW_FILE_NAME ì‚¬ìš©)
    file_path = os.path.join(FILE_DIR, RAW_FILE_NAME)
    df = pd.read_csv(file_path)
    df1 = df.copy()

    # 0) event_time ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°€ì¥ ë¨¼ì €!)
    df1 = sort_by_event_time(df1)

    # 1) event_time ì œê±°
    df1 = drop_event_time(df1)

    # 2) event_type / event_label / target ê²€ì¦ & ì •ë¦¬
    df1 = validate_and_cleanup_event_columns(df1)

    # 3) target 0/1/2 â†’ 0/0.5/1 ë³€í™˜
    df1 = convert_target_to_softlabel(df1)

    # 4) object ì»¬ëŸ¼ ì •ì œ
    df1 = clean_object_columns(df1)

    # 5) ìˆ˜ì¹˜í˜• ìŠ¤ì¼€ì¼ë§ (price logë³€í™˜ + MinMax, ë‹¤ë¥¸ ìˆ˜ì¹˜í˜• MinMax, is_weekend(0/1) ì œì™¸)
    df1 = scale_numerical_features(df1)

    # 6) ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ 6ê°œ ì¸ë±ì‹± + ë§¤í•‘ í”¼í´ ì €ì¥
    df1, mappings = build_and_apply_mappings(df1, drop_original=True)

    # 7) seq_prod ì¸ë±ì‹± (ê°€ì¥ ê³¼ê±° â†’ 1, ì§ì „ í–‰ë™ â†’ 5)
    df1 = apply_seq_prod_mapping(df1, mappings, seq_col="seq_prod", max_len=5)

    # === 8) ìµœì¢… ì „ì²˜ë¦¬ ê²°ê³¼ parquet ì €ì¥ ===
    save_preprocessed_parquet(df1, FILE_DIR, filename="final_preprocessed_df.parquet")

    # 9) Split ì—¬ê¸°ì„œëŠ” 80 : 10 : 10 ê¸°ì¤€ (AutoInt ë…¼ë¬¸ê³¼ ë™ì¼)
    train_df, valid_df, test_df = split_exact_tvt_from_preprocessed(
        random_state=42
    )
    print("[INFO] Split completed. Ready to build DataLoaders or Models.")

    # 10) DataLoader ìƒì„±
    train_loader, valid_loader, test_loader, feature_cols = create_dataloaders_from_df(
        train_df, valid_df, test_df
    )
    print("[INFO] DataLoaders created.")

batch_x, batch_y = next(iter(train_loader))
print(batch_x.shape)
print(batch_y.shape)

"""PKL ê²€ì¦ ìŠ¤ë‹ˆí«"""

import pickle
import os
import pandas as pd

full_file_path = os.path.join(FILE_DIR, SAVE_NAME)

# í”¼í´ ë¡œë“œ
with open(full_file_path, "rb") as f:
    id_maps = pickle.load(f)

print("=== í”¼í´ íŒŒì¼ ë¡œë“œ ì„±ê³µ ===")
print(f"ì €ì¥ëœ ì»¬ëŸ¼: {list(id_maps.keys())}")

print("\n======================================")
print("ğŸ” 1) ì• 20ê°œ ë§¤í•‘ í™•ì¸")
print("======================================")
for col, mapping in id_maps.items():
    print(f"\n[{col}] ì• 20ê°œ:")
    items = list(mapping.items())[:20]
    for k, v in items:
        print(f"  {k} -> {v}")

print("\n======================================")
print("ğŸ” 2) unknown = 0 í™•ì¸")
print("======================================")
for col in ["category_code", "brand"]:
    if "unknown" in id_maps[col]:
        print(f"{col}: unknown â†’ {id_maps[col]['unknown']}")
    else:
        print(f"{col}: unknown ì—†ìŒ (ì •ìƒ)")

print("\n======================================")
print("ğŸ” 3) ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸")
print("======================================")
for col, mapping in id_maps.items():
    vals = list(mapping.values())
    print(f"{col}: ìµœì†Œ={min(vals)}, ìµœëŒ€={max(vals)}, ê°œìˆ˜={len(vals)}")

print("\n======================================")
print("ğŸ” 4) ì¤‘ë³µ ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€")
print("======================================")
for col, mapping in id_maps.items():
    vals = list(mapping.values())
    if len(vals) != len(set(vals)):
        print(f"âš ï¸ {col}: ì¸ë±ìŠ¤ ì¤‘ë³µ ìˆìŒ!")
    else:
        print(f"âœ… {col}: ì¸ë±ìŠ¤ ì¤‘ë³µ ì—†ìŒ (ì •ìƒ)")

print("\n======================================")
print("ğŸ” 5) DataFrame í˜•íƒœë¡œ ë¯¸ë¦¬ë³´ê¸° (ì• 20ê°œ)")
print("======================================")
for col, mapping in id_maps.items():
    print(f"\n[{col}] ë§¤í•‘ í…Œì´ë¸” ë¯¸ë¦¬ë³´ê¸° 20ê°œ:")
    df_map = pd.DataFrame(list(mapping.items()), columns=[col, col + "_idx"])
    display(df_map.head(20))
    print("-" * 60)